# Robot Self-Modeling: Intermediate Progress Report

**Project:** Robot Self-Simulation (Nature Machine Intelligence Implementation)
**Date:** November 20, 2025

## 1. Introduction
This project is about teaching a robot to understand its own body. Usually, robots need engineers to tell them exactly how long their arms are and how they move (using URDF files). We are trying to make the robot learn this by itself, just by looking at itself. We are implementing the method from the paper "Teaching Robots to Build Simulations of Themselves" (2025).

The main idea is to use a neural network (similar to NeRF) that takes the robot's joint angles and predicts where its body is in 3D space.

## 2. What We Have Done So Far

We have successfully built the "brain" of the system and the training process. Here is a breakdown of our current status:

### The Neural Network (Completed)
We built the **Forward-Backward Visual Self-Model (FBV_SM)**. This is the core AI model.
- It takes two inputs: a point in 3D space (x, y, z) and the robot's motor commands (angles).
- It outputs a probability: "Is there a robot part at this location?"
- We used **Positional Encoding** (sine and cosine functions) to help the network see fine details. This is working correctly.

### The Training Loop (Completed)
We created a robust training system in `data/SelfSim.ipynb`.
- The code loads data, feeds it to the network, and updates the weights.
- We added **Validation**: The system automatically checks its performance on data it hasn't seen before to make sure it's actually learning, not just memorizing.
- We added **Visualization**: We can generate images from the model's predictions. This lets us visually compare what the robot "imagines" versus what is actually there.

### The Data Problem and Solution
When we started, we tried to use the dataset provided with the paper. However, we discovered that the image files were corrupted (they were all black).
- **Our Fix**: Instead of getting stuck, we wrote a script to generate **synthetic data**. We created a simplified 2D stick-figure robot that moves based on joint angles.
- **Result**: We trained our model on this synthetic data. The results were excellent. The loss (error rate) dropped significantly, and the model successfully learned to reconstruct the shape of our stick-figure robot. This proves our code works.

## 3. What Remains to be Done

While the code works on simple data, we need to make it work for a realistic robot arm.

### Task 1: Realistic Data Generation with PyBullet
We cannot rely on stick figures forever. We need to generate realistic images of a 3D robot arm.
- **Plan**: We will use **PyBullet**, a physics simulator.
- **Action**: We need to write a script that loads our robot description (URDF), moves the robot, and takes virtual photos.

### Task 2: Better Motion with Lorenz System
To learn well, the robot needs to move its arm into many different positions.
- **Current Issue**: Standard movements (like spirals or random shaking) might not cover the space efficiently or might be too predictable.
- **The Solution**: We will implement the **Lorenz System**. This is a mathematical equation that produces "chaotic" but smooth paths (the famous butterfly shape).
- **Why**: Using Lorenz trajectories will help the robot explore its workspace more thoroughly than simple random movements. This will make the self-model more accurate.

### Task 3: Final Integration
Once we have the PyBullet data generated by the Lorenz paths, we will feed it into our existing training loop. Since the training loop is already proven to work, this step should be straightforward.

## 4. Summary of Results
- **Codebase**: Stable and functional.
- **Validation**: Implemented and verified.
- **Visualization**: Implemented and verified.
- **Next Major Step**: Replace synthetic data with PyBullet simulation data driven by Lorenz attractor trajectories.

We are confident that the system is ready for the final phase of the project.
