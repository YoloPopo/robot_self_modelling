\documentclass[11pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} % Standard professional font
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{titlesec}

% --- FORMATTING SETUP ---
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    citecolor=blue
}

\setlength{\parskip}{0.8em}
\setlength{\parindent}{0pt}

% --- DOCUMENT METADATA ---
\title{Thesis Proposal: SparseKin}
\author{Muhammad Zeeshan Asghar}
\date{November 18, 2025}

\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    \Large\textbf{NATIONAL RESEARCH UNIVERSITY HIGHER SCHOOL OF ECONOMICS}
    
    \vspace{0.5cm}
    \large Faculty of Computer Science\\
    Master’s Programme in Data Science
    
    \vspace{2.5cm}
    
    \Huge\textbf{Thesis Proposal}
    
    \vspace{1cm}
    \Large\textbf{SparseKin: Bio-Inspired Kinematic Adaptation via Evolutionary Pruning of One-Shot Supernets}
    
    \vspace{2.5cm}
    
    \begin{flushleft}
    \large
    \textbf{Student:}\\
    Muhammad Zeeshan Asghar\\
    M.S. Data Science (2nd Year)
    
    \vspace{0.5cm}
    
    \textbf{Proposed Supervisor:}\\
    Sabarathinam Srinivasan\\
    Assistant Professor, School of Data Analysis and Artificial Intelligence
    \end{flushleft}
    
    \vfill
    
    \large Date: November 18, 2025
    
\end{titlepage}

% --- MAIN CONTENT ---

\section{Abstract}
Biological nervous systems are characterized by high degrees of sparsity; neurons do not connect to every other neuron, creating modular pathways that allow for rapid local adaptation without catastrophic interference. In contrast, state-of-the-art robotic self-models, specifically the \textbf{Free-Form Kinematic Self-Model (FFKSM)} [1], rely on dense neural fields that are computationally heavy and slow to ``heal'' when the robot suffers physical damage.

This thesis proposes \textbf{SparseKin}, a bio-inspired architecture that combines **3D Gaussian Splatting** with **Evolutionary Topology Search** to enable real-time self-repair. Building on the candidate's prior framework, \textit{``Adaptive Synergy''} [2], we hypothesize that sparsity is a prerequisite for structural plasticity. Unlike prior methods that require retraining dense networks, SparseKin utilizes a **One-Shot Supernet** strategy to evolve a sparse kinematic mask in real-time. This allows the robot to (1) reduce inference latency on embedded hardware (NVIDIA Jetson), and (2) significantly accelerate adaptation to ``messy'' real-world failures by routing error gradients only to the affected kinematic sub-graph.

\section{Motivation and Background}

\subsection{The Challenge: Resilience in the Wild}
True autonomy requires agents that can adapt to hardware failure without human intervention. Current visual self-models [1] are revolutionary but computationally brittle; they rely on dense gradient updates that ``smear'' errors across the entire network. If a robot damages a finger, a dense model often catastrophically forgets the structure of the arm while trying to fix the finger. We aim to solve this via **Modular Sparsity**.

\subsection{Theoretical Foundation: Bridging Evolution and Self-Modeling}
This work unifies two distinct research streams:
\begin{enumerate}
    \item \textbf{Visual Self-Modeling:} We adopt the FFKSM concept but replace the slow NeRF backbone with **3D Gaussian Splatting (3DGS)** to ensure edge-feasibility. While recent works have applied 3DGS to robotics, they utilize dense networks; our contribution is the introduction of \textit{Evolutionary Sparsity} for damage adaptation.
    \item \textbf{Adaptive Synergy:} In the candidate's prior work [2], we demonstrated that Evolutionary Strategies (ES) can escape local optima in non-stationary environments. SparseKin applies this logic to \textit{topology}: we use ES to ``evolve'' the wiring diagram of the robot's self-model, allowing it to discover that ``Joint A no longer moves Link B'' after damage.
\end{enumerate}

\section{Research Questions}
\begin{enumerate}[label=\textbf{RQ\arabic*}]
    \item \textbf{(The Plasticity Hypothesis):} Does a sparse, modular kinematic network adapt to physical damage significantly faster than a dense baseline?
    \item \textbf{(The Modularity Control):} Does evolved sparsity outperform **random pruning** (at the same sparsity level), proving that the \textit{topology} of the connections matters?
    \item \textbf{(Edge Feasibility):} Can we achieve $>10$ FPS inference latency on an **NVIDIA Jetson Orin Nano** by pivoting from NeRFs to 3D Gaussian Splatting?
\end{enumerate}

\section{Methodology}
This research employs a **Hybrid Optimization Strategy** on the **PyTorch** and **NVIDIA Isaac** platforms.

\subsection*{Phase 1: Baseline \& Backbone Selection}
To address the computational bottleneck of NeRFs, we implement the self-model using **3D Gaussian Splatting (3DGS)**.
\begin{itemize}
    \item \textbf{Input:} Joint Angles $\theta$.
    \item \textbf{Output:} A cloud of 3D Gaussians (Position, Rotation, Opacity) representing the robot.
    \item \textbf{Baseline:} A dense FFKSM implemented with 3DGS.
\end{itemize}

\subsection*{Phase 2: One-Shot Supernet Training}
To avoid the cost of training thousands of candidate networks, we employ a **Single-Path One-Shot (SPOS)** Supernet strategy:
\begin{itemize}
    \item \textbf{The Supernet:} We train one large, over-parameterized network where every joint connects to every Gaussian cluster.
    \item \textbf{Stabilization Strategy:} We employ **Uniform Sampling** of paths combined with **Fairness Regularization** to ensure that all sub-graphs are trained equally, preventing the ``ranking correlation'' problem common in NAS [3].
    \item \textbf{Risk Mitigation:} Supernet training is estimated at $\sim150$ GPU-hours on an NVIDIA RTX 4090. To mitigate failure risks, we implement **automatic checkpointing every 10,000 iterations**.
\end{itemize}

\subsection*{Phase 3: Evolving the Topology (The Mask)}
Once the Supernet is pre-trained, we use a **Genetic Algorithm (GA)** to search for the optimal damage-adaptive mask:
\begin{itemize}
    \item \textbf{Encoding:} Binary Adjacency Matrix.
    \item \textbf{Hyperparameters:} Population $N=50$, Generations $T=50$.
    \item \textbf{Feasibility:} Evaluating a mask requires only a forward pass. Total search time is estimated at $\sim1.5$ hours (2,500 evaluations $\times$ 2s inference), fitting easily within a single day.
\end{itemize}

\subsection*{Phase 4: The ``Damage Recovery'' Stress Test}
We compare **four** models on a simulated 4-DOF arm in PyBullet under 5 distinct failure conditions (e.g., Locked Joint, Bent Link):
\begin{enumerate}
    \item \textbf{Dense FFKSM:} Fully connected (Standard Gradient Descent).
    \item \textbf{Small Dense Baseline:} A dense network scaled down to match the parameter count of the sparse models (Isolating Capacity vs. Structure).
    \item \textbf{Random SparseKin:} A network pruned randomly to 20\% density (Modularity Control).
    \item \textbf{Evolved SparseKin:} A network where the 20\% mask is found via Evolution.
\end{enumerate}
\textbf{Gradient Routing:} During adaptation, we \textit{freeze} the Supernet weights and fine-tune only the active subgraph selected by the evolved mask using mask-specific gradient accumulation.

\subsection*{Phase 5: Edge Deployment}
We deploy the inference loop on an **NVIDIA Jetson Orin Nano**.
\begin{itemize}
    \item \textbf{Target Metric:} $>10$ FPS inference speed (based on GauRast benchmarks [6] for embedded 3DGS).
    \item \textbf{Optimization:} Using TensorRT to accelerate the sparse Gaussian rasterization.
\end{itemize}

\section{Expected Contributions}
\begin{enumerate}
    \item \textbf{Algorithmic Novelty:} To our knowledge, a novel application of **One-Shot Architecture Search** to Kinematic Self-Modeling.
    \item \textbf{Scientific Rigor:} Empirical evidence distinguishing between ``Sparsity'' (fewer weights) and ``Modularity'' (smart wiring) via the random pruning and parameter-matched controls.
    \item \textbf{Open Source Artifact:} A reproducible library for ``Real-Time Self-Repair'' using 3DGS.
\end{enumerate}

\section{Timeline (6-Month Plan)}
\begin{table}[h]
\centering
\begin{tabular}{@{}cp{3cm}p{9cm}@{}}
\toprule
\textbf{Month} & \textbf{Focus} & \textbf{Milestone} \\ \midrule
1 & Architecture & Implement 3D Gaussian Splatting kinematic backbone. \\
2 & Supernet & Implement Single-Path Supernet training loop. (Est. 1 week calendar time; 6 days continuous GPU training + debugging). \\
3 & Evolution & Develop Genetic Algorithm for mask search (Topology). \\
4 & Experiments & Run Damage Recovery tests (Dense vs. Small vs. Random vs. Evolved). \\
5 & Hardware & Deploy on NVIDIA Jetson; profile latency/energy. \\
6 & Writing & Finalize thesis and submit. \\ \bottomrule
\end{tabular}
\end{table}

\section{References}
\begin{thebibliography}{99}

\bibitem{hu2024}
Hu, Y., Lin, J., \& Lipson, H. (2024). 
Teaching Robots to Build Simulations of Themselves. 
\textit{Nature Machine Intelligence}, 6, 123–130.

\bibitem{asghar2025}
Asghar, M. Z., \& Srinivasan, S. (2025). 
Adaptive Synergy: Unifying Deep Reinforcement, Meta, and Evolutionary Learning. 
\textit{Zenodo Preprint}.

\bibitem{guo2020}
Guo, Z., et al. (2020). 
Single Path One-Shot Neural Architecture Search with Uniform Sampling. 
\textit{ECCV 2020}.

\bibitem{kerbl2023}
Kerbl, B., et al. (2023). 
3D Gaussian Splatting for Real-Time Radiance Field Rendering. 
\textit{ACM Trans. Graph.}

\bibitem{bongard2006}
Bongard, J., Zykov, V., \& Lipson, H. (2006). 
Resilient Machines Through Continuous Self-Modeling. 
\textit{Science}, 314(5802).

\bibitem{lassanske2024}
Lassanske, M., et al. (2024).
GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting.
\textit{ACM SIGGRAPH Asia}.

\end{thebibliography}

\end{document}